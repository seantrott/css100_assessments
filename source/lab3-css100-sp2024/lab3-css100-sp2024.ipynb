{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19be4b52",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc8dda2aa9533e81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 3: CSS 100\n",
    "\n",
    "In this lab, you'll get more hands-on practice:\n",
    "\n",
    "- Using the `nltk` library to process text.  \n",
    "- Working with *corpora*.\n",
    "- Computing correlations and regressions. \n",
    "\n",
    "This will all be in the context of trying to build an algorithm to predict the **readability** of text. We will be working with the [CLEAR corpus](https://link.springer.com/article/10.3758/s13428-022-01802-x), which contains human measures of readability (`BT_easiness`) and automated measures (see Q1). We'll also to build our own measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d38cff7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1f5e6068a728755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Run this code!\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import scipy.stats as ss\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb322267",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45e703b31013cca0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4724, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Run this code!\n",
    "df_clear = pd.read_csv(\"data/CLEAR_corpus_final.csv\")\n",
    "df_clear.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42ca57",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67d6bc4155ed292f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Existing metrics\n",
    "\n",
    "In this section, you'll compare the utility of existing *metrics* of readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d0a9e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1011bd8588adae02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1. Use existing readability metrics\n",
    "\n",
    "The *Clear Corpus* (`df_clear`) has several **automated measures** of text difficulty:\n",
    "\n",
    "- `Flesch-Kincaid-Grade-Level`\n",
    "- `Automated Readability Index`\n",
    "- `SMOG Readability`\n",
    "\n",
    "It also has a human \"gold standard\" measure (`BT_easiness`). For this problem, calculate the **Pearson's correlation** between each of these measures and `BT_easiness`. Name them as follows:\n",
    "\n",
    "- `Flesch-Kincaid-Grade-Level` --> `fk_corr`\n",
    "- `Automated Readability Index` --> `ari_corr`\n",
    "- `SMOG Readability` --> `smog_corr`\n",
    "\n",
    "**Note**: You can use `scipy.stats.pearsonr` (`ss`) to calculate $r$; make sure to select the correlation value, not the p-value.\n",
    "\n",
    "**Note 2**: You can read more about how the measures all work [here](https://en.wikipedia.org/wiki/Readability#Popular_Readability_Formulas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad39f2ae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4a8833050e50cce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "fk_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['Flesch-Kincaid-Grade-Level'])[0]\n",
    "ari_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['Automated Readability Index'])[0]\n",
    "smog_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['SMOG Readability'])[0]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9e77dd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-20043231aa2bb9ea",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert fk_corr\n",
    "assert ari_corr\n",
    "assert smog_corr\n",
    "\n",
    "assert fk_corr < -.5\n",
    "assert ari_corr < -.4\n",
    "assert smog_corr < -.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafe763",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0bb24ff1817a2e1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Build your own measure!\n",
    "\n",
    "In this section, you'll build your own, progressively more sophisticated, measure of readability. Then you can compare this measure to the others to see which best predicts human judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fdc60",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef9e2a68dbbfa7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2. A simple, naive measure\n",
    "\n",
    "Write a function called `naive_difficulty`, which tries to estimate the *difficulty* of reading a text. This function should:\n",
    "\n",
    "- Calculate the average sentence length (`num_words`/`num_sentences`).  \n",
    "- Calculate the average word length, in characters (`num_characters`/`num_words`).  \n",
    "\n",
    "These should be added together to produce a final estimate.\n",
    "\n",
    "Don't hesitate to use `word_tokenize` and `sent_tokenize`! See the `assert` statements for guidance on how the function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9081e10b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2cb4acdde781585b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def naive_difficulty(text):\n",
    "    num_words = len(word_tokenize(text))\n",
    "    num_sentences = len(sent_tokenize(text))\n",
    "    num_characters = len(text)\n",
    "    \n",
    "    return num_words/num_sentences + num_characters/num_words\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dad1148",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-18e6dbe112be52ff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert naive_difficulty\n",
    "\n",
    "text1 = \"Hello world. Hello!\"\n",
    "assert round(naive_difficulty(text1)) == 6\n",
    "\n",
    "text2 = \"This is a test. This is only a test.\"\n",
    "assert round(naive_difficulty(text2)) == 9\n",
    "\n",
    "text3 = \"He had come very far and was excited for the day. This is not to say he was not nervous at all. He was quite anxious.\"\n",
    "assert round(naive_difficulty(text3)) == 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e3bb8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe65a2b8f7e5b76b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3. Evaluate your `naive_difficulty` measure\n",
    "\n",
    "Now, `.apply` this measure to every `Excerpt` in `df_clear`. Call the new column `naive_difficulty`.\n",
    "\n",
    "Then calculate Pearson's $r$ with `BT_easiness` and call this `naive_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e259e0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-223d232d504b5d00",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_clear['naive_difficulty'] = df_clear['Excerpt'].apply(lambda x: naive_difficulty(x))\n",
    "naive_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['naive_difficulty'])[0]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52647584",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dc2cb8150def4ba5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'naive_difficulty' in df_clear.columns\n",
    "assert naive_corr\n",
    "\n",
    "assert naive_corr < 0\n",
    "assert naive_corr < -.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598e5f2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19ec272b977e93a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4. Which measure is best?\n",
    "\n",
    "Compare the magnitude of each of your correlations. Which one is *most correlated* with `BT_easiness`? Write down your answer below as one of \"A\", \"B\", \"C\", or \"D\", and set this to `ans4`.\n",
    "\n",
    "A. `fk_corr`   \n",
    "B. `smog_corr`  \n",
    "C. `ari_corr`  \n",
    "D. `naive_corr`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e8d719",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08eb60dafc988675",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans4 = \"B\"\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db4a832b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1165ec471bf0523f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ans4 in ['A', 'B', 'C', 'D']\n",
    "assert ans4 == 'B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af44d55",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-004e34dd94b9e0ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5. Using *Age of Acquisition*\n",
    "\n",
    "Some psycholinguists have hypothesized that words that are **learned earlier** are easier to read. This suggests that the **average age of acquisition of words** in a text should be correlated with the readability of that text. \n",
    "\n",
    "Build a new function called `calculate_avg_aoa(text)`, which looks up all the words in a text in `word_to_aoa`, a dictionary mapping words to their age of acquisition from a database (see the code below).\n",
    "\n",
    "Desired function behavior:\n",
    "\n",
    "- Passage text should first be made *lowercase*.  \n",
    "- For each *word* in text, check whether it has an *exact match* in `word_to_aoa`.  \n",
    "  - If so, track the `AoA` of that word.\n",
    "  - If not, ignore it.\n",
    "- Calculate the average `AoA` of words you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a91309e5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6c31f3c5d7b9bd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Run this code!\n",
    "df_aoa = pd.read_csv(\"data/AoA.csv\")\n",
    "word_to_aoa = dict(zip(df_aoa['Word'], df_aoa['AoA']))\n",
    "word_to_aoa['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c35fdb53",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-db9716e3fec345f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def calculate_avg_aoa(text):\n",
    "    # Convert the text to lowercase to match the case of dictionary keys\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # List to hold the AoA of words found in the dictionary\n",
    "    aoas = []\n",
    "    \n",
    "    # Check each word against the dictionary\n",
    "    for word in words:\n",
    "        if word in word_to_aoa:\n",
    "            aoas.append(word_to_aoa[word])\n",
    "    \n",
    "    # Calculate the average AoA if there are any valid AoA values\n",
    "    if aoas:\n",
    "        avg_aoa = sum(aoas) / len(aoas)\n",
    "    else:\n",
    "        avg_aoa = 0  # Return 0 or an appropriate value if no words match\n",
    "    \n",
    "    return avg_aoa\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b3d74078",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9a2f07a9d0b6a419",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert calculate_avg_aoa\n",
    "\n",
    "\n",
    "text1 = \"Hello world. Hello!\"\n",
    "assert round(calculate_avg_aoa(text1)) == 4\n",
    "\n",
    "text2 = \"This is a test. This is only a test.\"\n",
    "assert round(calculate_avg_aoa(text2)) == 5\n",
    "\n",
    "text3 = \"He had come very far and was excited for the day. This is not to say he was not nervous at all. He was quite anxious.\"\n",
    "assert round(calculate_avg_aoa(text3)) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1373f8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8bd39833b2710e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6. Evaluate the predictive value of `AoA`\n",
    "\n",
    "Now, `.apply` `calculate_avg_aoa` to every `Excerpt` in `df_clear`. Call the new column `avg_aoa`.\n",
    "\n",
    "Then calculate Pearson's $r$ with `BT_easiness` and call this `aoa_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fe70ff57",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e01c33c2947ace1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_clear['avg_aoa'] = df_clear['Excerpt'].apply(lambda x: calculate_avg_aoa(x))\n",
    "aoa_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['avg_aoa'])[0]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4bc893dd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce7a4efc288a58bf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'avg_aoa' in df_clear.columns\n",
    "\n",
    "assert aoa_corr\n",
    "assert aoa_corr < -.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a7615",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3fe0ffa3d6975464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q7. Combine `AoA` with `naive_difficulty`\n",
    "\n",
    "Now, let's see how much we can improve our measures by *combining* them. The *wisest* approach to doing this requires building a *linear regression* model to see how each one independently relates to `BT_easiness`.  \n",
    "\n",
    "Use the `statsmodels` package (imported below as `sm`) to `.fit` an `ols` model predicting `BT_easiness ~ avg_aoa + naive_difficulty`. Save the params in `combined_params`.\n",
    "\n",
    "**Note**: This is review from CSS 2. If you're a little rusty using `statsmodels`, check out the [lecture from CSS 2](https://ucsd-css2.github.io/ucsd-css2-website/lectures/13-regression-intro.html#building-a-regression-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a00824a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-030d49f30deea57f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "93e6a56a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d4603e5b4efdb2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "mod_combined = sm.ols(data = df_clear, formula = 'BT_easiness ~ avg_aoa + naive_difficulty').fit()\n",
    "combined_params = mod_combined.params\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c5a2b8a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7f8b0e4a9bd11131",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert 'avg_aoa' in combined_params\n",
    "assert 'naive_difficulty' in combined_params\n",
    "\n",
    "assert round(combined_params['avg_aoa'], 2) == -1\n",
    "assert round(combined_params['naive_difficulty'], 2) == -.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c2574",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b2395002c947f68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q8. Use `combined_params` to build a new algorithm\n",
    "\n",
    "We can use the **coefficients** from Q7 to build a new algorithm that estimates readability from `naive_difficulty` and `avg_aoa`. Specifically, you can think of these coefficients as *weights*, which you'll multiply by the respective measures:\n",
    "\n",
    "$Y = \\beta_{aoa} * X_{aoa} + \\beta_{naive}*X_{naive}$\n",
    "\n",
    "Write a function called `calculate_combined_readability(Excerpt)`, which:\n",
    "\n",
    "- First calculates `naive_difficulty` and `avg_aoa`.  \n",
    "- Then produces a **weighted** sum of these measures, using the coefficients of Q7.\n",
    "\n",
    "You can use the *rounded* versions of the coefficients available in the `assert` test for Q7. Specifically, $\\beta_{aoa}=-1$ and $\\beta_{naive}=-.03$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ffaa113",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa02dc610f51383",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def calculate_combined_readability(text):\n",
    "    β_aoa = -1\n",
    "    β_naive = -.03\n",
    "    \n",
    "    X_aoa = calculate_avg_aoa(text)\n",
    "    X_naive = naive_difficulty(text)\n",
    "    \n",
    "    Y = β_aoa * X_aoa + β_naive * X_naive\n",
    "    return Y\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c93dcee5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-97a95f01fb212ffe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert calculate_combined_readability\n",
    "\n",
    "text1 = \"Hello world. Hello!\"\n",
    "assert round(calculate_combined_readability(text1)) == -4\n",
    "\n",
    "text2 = \"This is a test. This is only a test.\"\n",
    "assert round(calculate_combined_readability(text2)) == -5\n",
    "\n",
    "text3 = \"He had come very far and was excited for the day. This is not to say he was not nervous at all. He was quite anxious.\"\n",
    "assert round(calculate_combined_readability(text3)) == -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd938087",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d823b55e3239541",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q8. Evaluate the predictive value of this new measure\n",
    "\n",
    "Now, `.apply` `calculate_combined_readability` to every `Excerpt` in `df_clear`. Call the new column `combined_readability`.\n",
    "\n",
    "Then calculate Pearson's $r$ with `BT_easiness` and call this `combined_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3adadf6c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f6f7b2643b5bc6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_clear['combined_readability'] = df_clear['Excerpt'].apply(lambda x: calculate_combined_readability(x))\n",
    "combined_corr = ss.pearsonr(df_clear['BT_easiness'],\n",
    "                     df_clear['combined_readability'])[0]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f4c27611",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6dc2166fa8dd0a06",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'combined_readability' in df_clear.columns\n",
    "\n",
    "assert combined_corr\n",
    "assert combined_corr > 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c6f8c",
   "metadata": {},
   "source": [
    "## Submit!\n",
    "\n",
    "Congratulations! You just built an **automated measure** of readability that out-performs many of the basic measures.\n",
    "\n",
    "By combining psycholinguistic variables with *automated variables*, we can come closer to estimating readability successfully.\n",
    "\n",
    "Don't forget to 'validate' and then 'submit'!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
