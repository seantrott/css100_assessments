{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555f928d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a63b85e8be80bd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 8: CSS 100\n",
    "\n",
    "In this lab, you'll get hands-on practice using the `transformers` package.\n",
    "\n",
    "- Tokenizing input strings.\n",
    "- Passing input to a pre-trained large language model.\n",
    "- Obtaining **logits** or predictions from the LLM.\n",
    "\n",
    "**Note**: To use `transformers`, make sure you're using the *Machine Learning* container for your DataHub account. You might need to relaunch your server to select this (\"Control panel --> stop my server --> relaunch\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adf58edd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-048c5317d6713f77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8b481",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fd84ada2ff2f5cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Tokenization\n",
    "\n",
    "**Tokenizers** are in charge of preparing text input for the LLM.\n",
    "\n",
    "For example, given a string like \"I like my coffee with cream and sugar\", a tokenizer will convert each word (or more accurately, each token) to a numeric representation that the LLM can use.\n",
    "\n",
    "We're going to work with GPT-2 today, so we need to load the tokenizer that GPT-2 uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0d69dd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6e06e054b726d83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bdfc56848e4370a440d57d4fa482b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326d5b29a647413c8d71b9abcdb84d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d523cb8c6cf4c19a0422b30f0e14005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bbc4dc7d4a4e38b3a9373ab767b36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca41ba1d7724755bb18e0f4002e05f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Run this code!\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fc0bb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9dea0060ab433672",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1. Count the tokens\n",
    "\n",
    "Use the `tokenizer` object to convert each of the `sentences` below to a list of tokens. Then, save the `count` of the number of tokens in each case to a list called `token_counts` (where the *index* should correspond to the number of tokens in that sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8de91df",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46bf483295767d17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Large language models work by predicting upcoming words\",\n",
    "    \"These models technically operate over tokens\",\n",
    "    \"Some particularly long words may have multiple tokens\",\n",
    "    \"And other short or frequent words use only a single token\",\n",
    "    \"You have to tokenize the word to find out\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fec10aca",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61b14aacea5e6a14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "token_counts = []\n",
    "for i in sentences:\n",
    "    tokens = tokenizer(i)\n",
    "    token_counts.append(len(tokens['input_ids']))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f269fdea",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-29ad15c28016b016",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## At least one hidden test\n",
    "assert len(token_counts) == 5\n",
    "assert token_counts[0] == 8\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert token_counts[1] == 6\n",
    "assert token_counts[2] == 8\n",
    "assert token_counts[3] == 11\n",
    "assert token_counts[4] == 10\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227593c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7991b3c27be7ec70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2. Words to tokens, and back to words\n",
    "\n",
    "> Each \"token\" in the LLM's vocabulary has its own ID.\n",
    "\n",
    "The `.convert_ids_to_tokens` function takes a `list` of **Token IDs** and converts it back to a string representation of that token. In this problem:\n",
    "\n",
    "- first *tokenize* each sentence in `sentences` using `tokenizer.encode(sentence))`\n",
    "- then, *convert* the token ids back to a string using `convert_ids_to_tokens`.\n",
    "- save these converted strings in a list called `token_strings`.\n",
    "\n",
    "**Reflect**: What do you notice about these decoded strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6abf9f96",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a270ad33d6eedb9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "token_ids = [tokenizer.encode(s) for s in sentences]\n",
    "token_strings = [tokenizer.convert_ids_to_tokens(t) for t in token_ids]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a54d6a3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6eeaffbb6ce12fc4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(token_strings) == 5\n",
    "assert token_strings[0] == ['Large',\n",
    " 'Ġlanguage',\n",
    " 'Ġmodels',\n",
    " 'Ġwork',\n",
    " 'Ġby',\n",
    " 'Ġpredicting',\n",
    " 'Ġupcoming',\n",
    " 'Ġwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57120cb4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44f4951deed59219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3. Identifying multi-token words\n",
    "\n",
    "This problem is more **open-ended**. Try to input different *words* to the `tokenizer` and see which individual words are tokenized into multiple tokens. See how many you can find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c6748cd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2288e193255ed971",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "multi_token_words = ['vanguard', 'vehicular', 'dehumidifier']\n",
    "\n",
    "w_ids = [tokenizer.encode(s) for s in multi_token_words]\n",
    "w_strings = [tokenizer.convert_ids_to_tokens(t) for t in w_ids]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85dc4db",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3ca87b8f311ebd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2. Using pre-trained LLMs!\n",
    "\n",
    "A **pre-trained language model** is one that has already been trained on a large corpus of text. The transformers library allows you to load a pre-trained model. This section introduces you to loading pre-trained models and doing basic operations with them.\n",
    "\n",
    "We'll be working with GPT-2, a pre-trained language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f836b5c7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-476fc53f2d7de1d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badc80c4daf5400a81409cd7753e6785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cdfff4f46a4fe59a4be2bbd8ff8379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Run this code!\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Load the model\n",
    "gpt2.eval()  # Put the model in \"evaluation mode\" (as opposed to training mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb4c21",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c62364148e85b347",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4. Pass input into the model\n",
    "\n",
    "In this problem, I'll give you a **sentence fragment**. Your job is to:\n",
    "\n",
    "- Tokenize that sentence fragment. \n",
    "- Pass the tokens into the pre-trained LLM (`gpt2`). \n",
    "- Set the result of that to a variable called `outputs`.\n",
    "\n",
    "Note that you can use the following code to pass input into an LLM object:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2(**inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e472c4c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-283665aa073c8760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentence_example = \"This class is really\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6385e6e8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1237030f82b74d4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "inputs = tokenizer(sentence_example, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2(**inputs)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b97e3878",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e6eac9bcce21cbbd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'logits' in outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5ccd6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd1c74840867b663",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5. Identify the top-$5$ predicted words\n",
    "\n",
    "The `outputs` you just created have a key called `logits`, which correspond to the model's **predictions**.\n",
    "\n",
    "> The `logits` are a series of values for each token in our model's vocabulary, at each position in the sequence of input tokens.\n",
    "\n",
    "We can use `outputs.logits.shape` to figure out the dimensions of this structure.\n",
    "\n",
    "The dimensions represent `(batch_size, sequence_length, vocab_size)`.\n",
    "\n",
    "- Batch size: The number of inputs we passed the model. Here, just 1.\n",
    "- Sequence length: The number of tokens in our sequence (4).\n",
    "- Vocab size: the number of tokens in the models vocabulary.\n",
    "\n",
    "The predictions for the **next** token can be found in the *final* token, i.e.,\n",
    "\n",
    "```python\n",
    "last_token_logits = sentence_logits[-1]\n",
    "```\n",
    "\n",
    "Return the **top five** predicted token IDs using these logits. Set this equal to `top_five_token_ids`.\n",
    "\n",
    "**Hint**: `torch.topk` can be of use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1abfd69",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96d33c98a0d0e196",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We're only looking at one sentence, so focus on that\n",
    "sentence_logits = outputs.logits[0]\n",
    "\n",
    "### Get last token logits.\n",
    "last_token_logits = sentence_logits[-1]\n",
    "last_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d696a119",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6eeece6ade12565",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 922, 4465,  546, 1593, 1049])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_five_token_ids = torch.topk(last_token_logits, k = 5).indices\n",
    "top_five_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6eb1079",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fb5a74718afad3ef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(top_five_token_ids) == 5\n",
    "assert sorted(top_five_token_ids.tolist()) == [546, 922, 1049, 1593, 4465]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2196ea",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-977343a8404a36cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6. Now, `decode` those words!\n",
    "\n",
    "Unless you're secretly a tokenizer, you don't know what **tokens** those IDs correspond to. So let's use `tokenizer.decode` to convert them all back to words. Set the result to `top_five_tokens`.\n",
    "\n",
    "**Reflect**: Do those words make sense in this context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24ccc156",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f96f9ea1e5b61324",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "top_five_tokens = [tokenizer.decode(t) for t in top_five_token_ids]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c551f18",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8382782319ec3d7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(top_five_tokens) == 5\n",
    "assert top_five_tokens == [' good', ' useful', ' about', ' important', ' great']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ec87b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe0fcdf1a450a6f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q7. Write a `sample_words` function\n",
    "\n",
    "Now that you know how to identify the most probable tokens, write a function called `sample_words`. This function will:\n",
    "\n",
    "- Take as *input* a `sentence_fragment`, a trained `model` (e.g., `gpt2`), a `tokenizer`, and an integer `k`.\n",
    "- Identify the top `k` *most likely* next words, based on the model.\n",
    "- Return those words in a `list`.\n",
    "\n",
    "**Reflect**: How might you use a function like this to generate entire paragraphs of text? (Hint: It won't just be by increasing `k`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4ff7ae7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-227c2eead054f387",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def sample_words(sentence_fragment, model, tokenizer, k):\n",
    "    \n",
    "    ### Tokenize\n",
    "    inputs = tokenizer(sentence_fragment, return_tensors=\"pt\")\n",
    "    \n",
    "    ### Get outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2(**inputs)\n",
    "        \n",
    "    ### We're only looking at one sentence, so focus on that\n",
    "    sentence_logits = outputs.logits[0]\n",
    "\n",
    "    ### Get last token logits.\n",
    "    last_token_logits = sentence_logits[-1]\n",
    "    \n",
    "    ### Most likely k token IDs\n",
    "    top_k_token_ids = torch.topk(last_token_logits, k = k).indices\n",
    "    \n",
    "    ### Decode original tokens\n",
    "    top_k_tokens = [tokenizer.decode(t) for t in top_k_token_ids]\n",
    "    \n",
    "    return top_k_tokens\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9394a47",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7c899f6760a8daf8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Test 1\n",
    "sentence_fragment = \"I like my coffee with cream and\"\n",
    "assert sample_words(sentence_fragment, gpt2, tokenizer, 1) == [\" cream\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9fb09340",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d8d2413e9de299cb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Test 2\n",
    "sentence_fragment = \"I think that large language models are very\"\n",
    "assert sorted(sample_words(sentence_fragment, gpt2, tokenizer, 3)) == [' good', ' important', ' useful']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d86c82",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35430c3a91005d94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q8. Calculate the probability of specific tokens\n",
    "\n",
    "Finally, we're often interested in calculating the probability of a **specific upcoming token** in a sequence.\n",
    "\n",
    "To do this, we'll need to first `softmax` the `logits`, and then figure out the probability assigned to the token ID in question. We've written a handy function for you called `next_seq_prob` which does this already.\n",
    "\n",
    "In this final problem, we want you to **use** this function to calculate the probability assigned to each word in the `candidates` list below, given the `sentence` fragment. Save your responses in a `dict` mapping each candidate to a probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b808f017",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd280108b60b4745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def next_seq_prob(model, tokenizer, seen, unseen):\n",
    "    \"\"\"Get p(unseen | seen)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : transformers.PreTrainedModel\n",
    "        Model to use for predicting tokens\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        Tokenizer for Model\n",
    "    seen : str\n",
    "        Input sequence\n",
    "    unseen: str\n",
    "        The sequence for which to calculate a probability\n",
    "    \"\"\"\n",
    "    # Get ids for tokens\n",
    "    input_ids = tokenizer.encode(seen, return_tensors=\"pt\")\n",
    "    unseen_ids = tokenizer.encode(unseen)\n",
    "\n",
    "    # Loop through unseen tokens & store log probs\n",
    "    log_probs = []\n",
    "    for unseen_id in unseen_ids:\n",
    "\n",
    "        # Run model on input\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "\n",
    "        # Get next token prediction logits\n",
    "        next_token_logits = logits[0, -1]\n",
    "        next_token_probs = torch.softmax(next_token_logits, 0) # Normalize\n",
    "\n",
    "        # Get probability for relevant token in unseen string & store\n",
    "        prob = next_token_probs[unseen_id]\n",
    "        log_probs.append(torch.log(prob))\n",
    "\n",
    "        # Add input tokens incrementally to input\n",
    "        input_ids = torch.cat((input_ids, torch.tensor([[unseen_id]])), 1)\n",
    "\n",
    "    # Add log probs together to get total log probability of sequence\n",
    "    total_log_prob = sum(log_probs)\n",
    "    # Exponentiate to return to probabilities\n",
    "    total_prob = torch.exp(total_log_prob)\n",
    "    return total_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d0c7b58",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3aff947431c23f5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Sentence and candidates\n",
    "sentence = \"I like my coffee with cream and\"\n",
    "candidates = [\" sugar\", \" honey\", \" mercury\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99246dc2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93c1076d50ca693e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "results = {}\n",
    "for candidate in candidates:\n",
    "    prob = next_seq_prob(gpt2, tokenizer, sentence, candidate)\n",
    "    results[candidate] = prob\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fa63a9a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0061075376814003",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert results\n",
    "assert ' sugar' in results\n",
    "assert ' honey' in results\n",
    "assert ' mercury' in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eefceba5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca17c6dfbd5809fe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(results[' sugar'], 2) == 0.05\n",
    "assert round(results[' honey'], 2) == 0.02\n",
    "assert round(results[' mercury'], 2) == 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba03b7e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e82695fe32cb643f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Submit!\n",
    "\n",
    "Congratulations, you just learned how to work with an LLM in `transformers`! That's a state-of-the-art package for using LLMs in Python."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
